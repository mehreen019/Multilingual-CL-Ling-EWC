{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistically-Aware EWC for Multilingual Continual Learning\n",
    "\n",
    "**Research Question**: Does explicit linguistic similarity improve parameter preservation in multilingual continual learning?\n",
    "\n",
    "## Experiment Setup\n",
    "- **Languages**: Bengali â†’ Hindi\n",
    "- **Task**: Sentiment Analysis\n",
    "- **Dataset**: ai4bharat/IndicSentiment\n",
    "- **Model**: XLM-RoBERTa-base\n",
    "- **Methods**: Naive, Standard EWC, Linguistic EWC, Random EWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets scikit-learn matplotlib seaborn tqdm\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository (or upload files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Clone from GitHub (if you've pushed the code)\n",
    "# !git clone https://github.com/yourusername/multilingual-cl-ling-ewc.git\n",
    "# %cd multilingual-cl-ling-ewc\n",
    "\n",
    "# Option 2: Upload files manually\n",
    "# - Click folder icon on left\n",
    "# - Upload: config.py, dataset.py, ewc.py, evaluation.py, train.py, visualize.py\n",
    "\n",
    "# Verify files are present\n",
    "!ls -la *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading IndicSentiment dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading Bengali dataset...\")\n",
    "dataset_bn = load_dataset(\"ai4bharat/IndicSentiment\", \"bn\", split=\"train\", trust_remote_code=True)\n",
    "print(f\"Bengali samples: {len(dataset_bn)}\")\n",
    "\n",
    "print(\"\\nLoading Hindi dataset...\")\n",
    "dataset_hi = load_dataset(\"ai4bharat/IndicSentiment\", \"hi\", split=\"train\", trust_remote_code=True)\n",
    "print(f\"Hindi samples: {len(dataset_hi)}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Bengali review:\")\n",
    "print(\"=\"*60)\n",
    "sample = dataset_bn[0]\n",
    "print(f\"Text: {sample.get('INDIC REVIEW', 'N/A')[:200]}...\")\n",
    "print(f\"Label: {sample.get('LABEL', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Hindi review:\")\n",
    "print(\"=\"*60)\n",
    "sample = dataset_hi[0]\n",
    "print(f\"Text: {sample.get('INDIC REVIEW', 'N/A')[:200]}...\")\n",
    "print(f\"Label: {sample.get('LABEL', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ExperimentConfig\n",
    "\n",
    "# Create configuration\n",
    "config = ExperimentConfig(\n",
    "    model_name=\"xlm-roberta-base\",\n",
    "    num_epochs_per_language=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    ewc_lambda=5000.0,\n",
    "    fisher_sample_size=1000,\n",
    "    bengali_hindi_similarity=0.6,\n",
    "    train_size=2000,  # Adjust based on available data\n",
    "    eval_size=400,\n",
    "    random_seed=42,\n",
    "    output_dir=\"./results\"\n",
    ")\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Epochs per language: {config.num_epochs_per_language}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"EWC lambda: {config.ewc_lambda}\")\n",
    "print(f\"Bengali-Hindi similarity: {config.bengali_hindi_similarity}\")\n",
    "print(f\"Train size: {config.train_size}\")\n",
    "print(f\"Methods: {config.methods}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Experiment\n",
    "\n",
    "This will train 4 different methods:\n",
    "1. **Naive**: No continual learning (baseline)\n",
    "2. **Standard EWC**: Fixed penalty\n",
    "3. **Linguistic EWC**: Similarity-scaled penalty (our method)\n",
    "4. **Random EWC**: Random scaling (sanity check)\n",
    "\n",
    "**Expected runtime**: ~30-60 minutes on GPU (T4), longer on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main experiment\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python visualize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load comparison results\n",
    "with open('./results/comparison.json', 'r') as f:\n",
    "    comparison = json.load(f)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENTAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for method in ['naive', 'ewc', 'ling_ewc', 'random_ewc']:\n",
    "    if method in comparison:\n",
    "        summary = comparison[method]\n",
    "        print(f\"\\n{method.upper().replace('_', ' ')}:\")\n",
    "        print(f\"  Average Forgetting: {summary['average_forgetting']:.4f}\")\n",
    "        print(f\"  Final Avg Accuracy: {summary['average_final_accuracy']:.4f}\")\n",
    "        print(f\"  Bengali Final: {summary['final_performance'].get('bengali', 0):.4f}\")\n",
    "        print(f\"  Hindi Final: {summary['final_performance'].get('hindi', 0):.4f}\")\n",
    "\n",
    "# Show improvement\n",
    "if 'ling_ewc_improvement_over_ewc' in comparison:\n",
    "    improvement = comparison['ling_ewc_improvement_over_ewc']\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY FINDING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Linguistic EWC vs Standard EWC:\")\n",
    "    print(f\"  Reduction in forgetting: {improvement:.4f}\")\n",
    "    \n",
    "    if improvement > 0.01:\n",
    "        print(f\"\\n  âœ… RESULT: Linguistic scaling REDUCES forgetting by {improvement:.2%}\")\n",
    "        print(f\"     â†’ Linguistic structure matters for continual learning!\")\n",
    "    elif improvement < -0.01:\n",
    "        print(f\"\\n  âŒ RESULT: Linguistic scaling INCREASES forgetting by {abs(improvement):.2%}\")\n",
    "        print(f\"     â†’ May need to adjust scaling strategy\")\n",
    "    else:\n",
    "        print(f\"\\n  âž– RESULT: No significant difference\")\n",
    "        print(f\"     â†’ Fisher Information may already capture linguistic structure\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visualizations\n",
    "print(\"\\nðŸ“Š Forgetting Comparison:\")\n",
    "display(Image('./results/forgetting_comparison.png'))\n",
    "\n",
    "print(\"\\nðŸ“Š Final Accuracy:\")\n",
    "display(Image('./results/final_accuracy.png'))\n",
    "\n",
    "print(\"\\nðŸ“Š Learning Curves:\")\n",
    "display(Image('./results/learning_curves.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip results for download\n",
    "!zip -r results.zip ./results/\n",
    "\n",
    "print(\"Results zipped! Download 'results.zip' from the files panel.\")\n",
    "print(\"\\nContents:\")\n",
    "print(\"  - comparison.json (numerical results)\")\n",
    "print(\"  - *_metrics.json (per-method detailed metrics)\")\n",
    "print(\"  - *.png (visualization plots)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistical Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For publishable results, run multiple seeds\n",
    "import numpy as np\n",
    "\n",
    "print(\"To get statistically significant results:\")\n",
    "print(\"1. Run experiment with multiple seeds (5-10 runs)\")\n",
    "print(\"2. Compute mean Â± std for all metrics\")\n",
    "print(\"3. Run t-test to check significance\")\n",
    "print(\"\\nExample code:\")\n",
    "print(\"\"\"\n",
    "for seed in [42, 43, 44, 45, 46]:\n",
    "    config.random_seed = seed\n",
    "    config.output_dir = f'./results_seed_{seed}'\n",
    "    # Run experiment\n",
    "    ...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Interpretation Guide\n",
    "\n",
    "**If Linguistic EWC reduces forgetting more than Standard EWC:**\n",
    "- âœ… Your hypothesis is supported\n",
    "- âœ… Linguistic similarity helps continual learning\n",
    "- âœ… Ready for publication (after multiple seeds)\n",
    "\n",
    "**If no significant difference:**\n",
    "- Still valuable! Negative results are publishable\n",
    "- Shows Fisher Information is linguistically-aware\n",
    "- Suggests explicit bias unnecessary\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **More languages**: Add Marathi, Tamil for 3-4 language sequence\n",
    "2. **Distance pairs**: Test Hindiâ†’Tamil (distant) vs Hindiâ†’Marathi (close)\n",
    "3. **Different tasks**: Try NER, POS tagging\n",
    "4. **Learned similarity**: Meta-learn optimal scaling\n",
    "5. **Statistical rigor**: Multiple seeds, significance tests\n",
    "\n",
    "### Citation\n",
    "\n",
    "If this helps your research, cite:\n",
    "- Kirkpatrick et al. (2017) - Original EWC\n",
    "- AI4Bharat IndicSentiment dataset\n",
    "- Your paper (once published!)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
